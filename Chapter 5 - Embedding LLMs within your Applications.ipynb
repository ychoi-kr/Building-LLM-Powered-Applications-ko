{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 랭체인의 구성 요소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델과 프롬프트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "왜 곰은 냉장고를 열지 않을까? \n",
      "- 냉장고 안에 살던 냉동곰을 무서워해서!\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(openai_api_key=openai_api_key)\n",
    "print(llm.invoke('농담을 해봐.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장: 탁자 위에 고양이가 있다\n",
      "영어로 번역:\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"문장: {sentence}\n",
    "{language}로 번역:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"sentence\", \"language\"])\n",
    "\n",
    "print(prompt.format(sentence = \"탁자 위에 고양이가 있다\", language = \"영어\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 연결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문서 로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예제 CSV 파일 \"sample.csv\"를 만들었습니다.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    ['이름', '나이', '도시'],\n",
    "    ['존', 25, '뉴욕'],\n",
    "    ['에밀리', 28, '로스엔젤레스'],\n",
    "    ['미카엘', 22, '시카고']\n",
    "]\n",
    "\n",
    "# File name\n",
    "file_name = 'sample.csv'\n",
    "\n",
    "# 데이터를 CSV 파일에 기록\n",
    "with open(file_name, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerows(data)\n",
    "\n",
    "print(f'예제 CSV 파일 \"{file_name}\"를 만들었습니다.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'sample.csv', 'row': 0}, page_content='이름: 존\\n나이: 25\\n도시: 뉴욕'), Document(metadata={'source': 'sample.csv', 'row': 1}, page_content='이름: 에밀리\\n나이: 28\\n도시: 로스엔젤레스'), Document(metadata={'source': 'sample.csv', 'row': 2}, page_content='이름: 미카엘\\n나이: 22\\n도시: 시카고')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path='sample.csv')\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문서 분할기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='고요한 풍경 속에서 우뚝 솟은 산들은 자연의 아름다움을 지키는 장엄한 수호자처럼 서 있습니다.'\n",
      "page_content='청량한 산 공기는 고요함의 속삭임을 전해주며, 바스락거리는 잎사귀들은 야생의 교향곡을 작곡합니다.\n",
      "자연의 팔레트는 산을 초록과 갈색의 색조로 칠해 경이로운 광경을 만들어냅니다.'\n",
      "page_content='해가 뜨면, 황금빛 광채가 산 정상에 비치며, 손길 닿지 않은 야생의 세계를 비춥니다.'\n"
     ]
    }
   ],
   "source": [
    "# 산과 자연에 대한 샘플 문장\n",
    "content = \"\"\"고요한 풍경 속에서 우뚝 솟은 산들은 자연의 아름다움을 지키는 장엄한 수호자처럼 서 있습니다.\n",
    "청량한 산 공기는 고요함의 속삭임을 전해주며, 바스락거리는 잎사귀들은 야생의 교향곡을 작곡합니다.\n",
    "자연의 팔레트는 산을 초록과 갈색의 색조로 칠해 경이로운 광경을 만들어냅니다.\n",
    "해가 뜨면, 황금빛 광채가 산 정상에 비치며, 손길 닿지 않은 야생의 세계를 비춥니다.\"\"\"\n",
    "\n",
    "# 파일명\n",
    "file_name = 'mountain.txt'\n",
    "\n",
    "# 텍스트 파일에 내용 쓰기\n",
    "with open(file_name, 'w') as txtfile:\n",
    "    txtfile.write(content)\n",
    "\n",
    "# 샘플 텍스트 파일 \"mountain.txt\" 생성 및 저장.\n",
    "# print(f'샘플 텍스트 파일 \"{file_name}\"를 만들었습니다.')\n",
    "\n",
    "with open('mountain.txt') as f:\n",
    "    mountain = f.read()\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([mountain])\n",
    "print(texts[0])\n",
    "print(texts[1])\n",
    "print(texts[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 임베딩 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베드된 문서:\n",
      "Number of vector: 5; Dimension of each vector: 1536\n",
      "임베드 질의:\n",
      "Dimension of the vector: 1536\n",
      "Sample of the first 5 elements of the vector: [-0.010634176433086395, -0.01016946416348219, -0.0020040736999362707, 0.023065242916345596, -0.026829415932297707]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model ='text-embedding-3-small' )\n",
    "\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"Good morning!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"I want to report an accident\",\n",
    "        \"Sorry to hear that. May I ask your name?\",\n",
    "        \"Sure, Mario Rossi.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"임베드된 문서:\")\n",
    "print(f\"Number of vector: {len(embeddings)}; Dimension of each vector: {len(embeddings[0])}\")\n",
    "\n",
    "embedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\")\n",
    "\n",
    "print(\"임베드 질의:\")\n",
    "print(f\"Dimension of the vector: {len(embedded_query)}\")\n",
    "print(f\"Sample of the first 5 elements of the vector: {embedded_query[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대화 텍스트 파일 \"dialogue.txt\"를 만들었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 대화를 txt 파일에 저장\n",
    "# 대화 행 목록\n",
    "dialogue_lines = [\n",
    "    \"Good morning!\",\n",
    "    \"Oh, hello!\",\n",
    "    \"I want to report an accident\",\n",
    "    \"Sorry to hear that. May I ask your name?\",\n",
    "    \"Sure, Mario Rossi.\"\n",
    "]\n",
    "\n",
    "# 파일명\n",
    "file_name = 'dialogue.txt'\n",
    "\n",
    "# 대화 행들을 텍스트 파일에 기록\n",
    "with open(file_name, 'w') as txtfile:\n",
    "    for line in dialogue_lines:\n",
    "        txtfile.write(line + '\\n')\n",
    "\n",
    "print(f'대화 텍스트 파일 \"{file_name}\"를 만들었습니다.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 벡터 스토어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yong\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# 문서를 로드해 청크들로 분할하고, 각 청크를 임베드해 벡터 스토어에 로드\n",
    "\n",
    "raw_documents = TextLoader('dialogue.txt').load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=50, chunk_overlap=0, separator = \"\\n\",)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "db = FAISS.from_documents(documents, OpenAIEmbeddings())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to report an accident\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the reason for calling?\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Sorry to hear that. May I ask your name?' metadata={'source': 'dialogue.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(documents[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검색기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The reason for the call was to report an accident.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "query = \"What was the reason of the call?\"\n",
    "output = qa.invoke(query)\n",
    "output['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 메모리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': '\\nThe human is looking for ideas to write an essay about AI. The AI suggests writing about LLM.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory, ChatMessageHistory\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=OpenAI(temperature=0))\n",
    "memory.save_context({\"input\": \"안녕하세요, AI에 관한 에세이를 쓸 아이디어를 찾고 있어요\"}, {\"output\": \"안녕하세요, LLM에 관해 써보면 어떨까요?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mConversationSummaryMemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Dict[str, Any]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Dict[str, str]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m'None'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m Save context from this conversation to buffer.\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\langchain\\memory\\summary.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "ConversationSummaryMemory.save_context?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 체인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단순 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yong\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' There is a cat on the table.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"문장: {sentence}\n",
    "{language}로 번역:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"sentence\", \"language\"])\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "llm_chain.predict(sentence=\"탁자 위에 고양이가 있어요\", language=\"영어\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There is a cat on the table.\n"
     ]
    }
   ],
   "source": [
    "# LLMChain deprecation 해결\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableSequence, RunnablePassthrough\n",
    "\n",
    "template = \"\"\"문장: {sentence}\n",
    "{language}로 번역:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"sentence\", \"language\"])\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = RunnableSequence(\n",
    "    {\n",
    "        \"sentence\": RunnablePassthrough(),\n",
    "        \"language\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "result = chain.invoke({\n",
    "    \"sentence\": \"탁자 위에 고양이가 있어요\",\n",
    "    \"language\": \"영어\"\n",
    "})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라우터 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yong\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use RunnableWithMessageHistory: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html instead.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\yong\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: Use RunnableLambda to select from multiple prompt templates. See example in API reference: https://api.python.langchain.com/en/latest/chains/langchain.chains.router.multi_prompt.MultiPromptChain.html\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "itinerary_template = \"\"\"당신은 휴가 일정 조수입니다. \\\n",
    "당신은 고객이 최고의 목적지와 일정을 찾도록 도와줍니다. \\\n",
    "당신은 고객의 선호에 따라 최적화된 일정을 작성하는 데 도움을 줍니다.\n",
    "\n",
    "여기에 질문이 있습니다:\n",
    "{input}\"\"\"\n",
    "\n",
    "restaurant_template = \"\"\"당신은 레스토랑 예약 조수입니다. \\\n",
    "당신은 고객의 손님 수와 음식 선호를 확인합니다. \\\n",
    "특별한 조건을 고려해야 하는지 주의합니다.\n",
    "\n",
    "여기에 질문이 있습니다:\n",
    "{input}\"\"\"\n",
    "\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"여행 일정\",\n",
    "        \"description\": \"여행 일정 작성을 돕습니다\",\n",
    "        \"prompt_template\": itinerary_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"레스토랑\",\n",
    "        \"description\": \"고객의 레스토랑 예약을 도와줍니다\",\n",
    "        \"prompt_template\": restaurant_template,\n",
    "    },\n",
    "]\n",
    "\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "default_chain = ConversationChain(llm=llm, output_key=\"text\")\n",
    "\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "여행 일정: {'input': '밀라노에서 베니스까지 자동차로 여행하려고 합니다. 중간에 들릴 만한 명소가 있나요?.'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': '밀라노에서 베니스까지 자동차로 여행하려고 합니다. 중간에 들릴 만한 명소가 있나요?.', 'text': '\\n\\n1. 밀라노에서 베니스까지의 여행 시간은 얼마나 걸리나요?\\n2. 자동차 여행으로 가는 것보다 기차나 비행기로 가는 것이 더 빠른가요?\\n3. 중간에 들릴 만한 명소는 어떤 곳이 있나요? 역사적인 곳이나 볼거리가 있는 곳 등\\n4. 자동차 여행으로 가는데 필요한 경비는 얼마나 되나요?\\n5. 여행 중에 주차할 수 있는 곳이 어디인가요?\\n6. 베니스에서 머물기 좋은 호텔이나 숙박 시설은 어디인가요?\\n7. 베니스에서 즐길 수 있는 여행 액티비티는 무엇이 있나요?\\n8. 베니스에서 밀라노'}\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"밀라노에서 베니스까지 자동차로 여행하려고 합니다. 중간에 들릴 만한 명소가 있나요?.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "레스토랑: {'input': '오늘 저녁 식사를 예약하고 싶어요'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': '오늘 저녁 식사를 예약하고 싶어요', 'text': '. 몇 명이서 오실 건가요? 몇 명의 성인과 어린이인지 알려주세요.\\n어떤 종류의 음식을 선호하시나요? 고기, 해산물, 채식 등 다양한 옵션 중에서 선택할 수 있습니다.\\n알레르기나 식이 요구 사항이 있으신가요? 예를 들어, 글루텐 또는 유제품 무첨가 요리를 원하시나요? 특별한 식이 요구 사항이 있다면 말씀해주세요.\\n어떤 특별한 행사를 기념하시나요? 생일, 결혼 기념일 등 특별한 날에 맞춰 서비스를 제공할 수 있습니다.\\n결제는 어떻게 하실 건가요? 현금, 카드,'}\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"오늘 저녁 식사를 예약하고 싶어요\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시퀀셜 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 이 코드는 연극의 제목을 주면 시놉시스를 작성하는 LLMChain입니다.\n",
    "llm = OpenAI(temperature=0)\n",
    "template = \"\"\"당신은 코미디언입니다. {topic}에 관한 농담을 생성하세요.\n",
    "농담:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"topic\"], template=template)\n",
    "joke_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "template = \"\"\"당신은 번역가입니다. 주어진 텍스트 입력을 {language}로 번역하세요.\n",
    "번역:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"language\"], template=template)\n",
    "translator_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yong\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m \"고양이와 개가 싸우면 누가 이길까요? 당연히 고양이죠, 개는 꼬리를 쫓는데 바쁘니까요!\"\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m If cats and dogs fight, who do you think will win? Of course, it's the cat, because the dog is too busy chasing its tail!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 이 코드는 두 체인을 순서대로 실행하는 전체 체인입니다.\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[joke_chain, translator_chain], verbose=True)\n",
    "translated_joke = overall_chain.run(\"고양이와 개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 변환 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string 모듈을 임포트합니다\n",
    "import string\n",
    "\n",
    "# 함수를 정의합니다\n",
    "def rename_cat(inputs: dict) -> dict:\n",
    "    # 파일을 읽기 모드로 엽니다\n",
    "    text = inputs[\"text\"]\n",
    "    # 'cat'을 'Silvester the Cat'으로 바꿉니다\n",
    "    new_text = text.replace('cat', 'Silvester the Cat')\n",
    "    # 변경된 텍스트를 반환합니다\n",
    "    return {\"output_text\": new_text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"\\nThe Cat and the Dog\\n\\nThere was once a cat and a dog who lived in the same house. They did not get along very well, as they often fought over food, toys, and attention. The cat was clever and cunning, while the dog was loyal and friendly. The cat liked to tease the dog, and the dog liked to chase the cat.\\n\\nOne day, the cat decided to play a prank on the dog. He found a ball of yarn and tied it around the dog's tail. Then he hid behind a sofa and waited for the dog to notice. When the dog saw the yarn, he thought it was a toy and started to play with it. He ran around the house, trying to catch the yarn with his mouth. But every time he got close, the yarn moved away from him. The cat laughed silently as he watched the dog's futile attempts.\\n\\nThe dog soon realized that something was wrong. He looked behind him and saw that the yarn was attached to his tail. He tried to pull it off, but it was too tight. He felt angry and embarrassed. He wondered who did this to him. He sniffed the air and detected the cat's scent. He knew it was the cat who tricked him. He growled and ran towards the sofa where the cat was hiding.\\n\\nThe cat heard the dog's growl and saw him coming. He panicked and ran away from the sofa. He hoped to find a safe place to hide, but he was too late. The dog was faster and caught up with him. He grabbed the cat by the scruff of his neck and shook him hard. The cat yowled and scratched the dog's face. The dog barked and bit the cat's ear. They rolled on the floor, biting and clawing each other.\\n\\nThe noise they made woke up their owner, who was sleeping upstairs. She came down and saw them fighting. She was shocked and angry. She shouted at them to stop. She grabbed a broom and hit them lightly on their heads. They stopped fighting and looked at her with fear. She scolded them for being naughty and making a mess. She untied the yarn from the dog's tail and threw it away. She took them to the bathroom and cleaned their wounds. She told them to behave themselves and get along.\\n\\nThe cat and the dog felt ashamed of themselves. They realized that they had hurt each other and their owner. They apologized to each other and to their owner. They promised to be nicer to each other and share their things. They hugged each other and licked each other's faces.\\n\\nFrom that day on, they became friends. They played together, slept together, and ate together. They learned to respect each other's differences and appreciate each other's strengths. They were happy and content.\\n\\nThe end.\",\n",
       " 'output': '\\n실베스터 고양이와 개는 같은 집에서 살았지만 음식, 장난감, 관심 등을 놓고 자주 싸웠다. 어느 날 고양이가 개를 속이기 위해 실로 묶어서 놀려주지만 결국 싸움을 하게 되고 주인에게 혼나게 된다. 그 후 둘은 서로를 존중하고 친구가 되어 행복하게 지냈다. '}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import TransformChain, LLMChain, SimpleSequentialChain\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "with open(\"Cats&Dogs.txt\") as f:\n",
    "    cats_and_dogs = f.read()\n",
    "\n",
    "\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "transform_chain = TransformChain(\n",
    "    input_variables=[\"text\"], output_variables=[\"output_text\"], transform=rename_cat\n",
    ")\n",
    "\n",
    "template = \"\"\"이 텍스트를 요약하세요:\n",
    "\n",
    "{output_text}\n",
    "\n",
    "요약:\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"output_text\"], template=template)\n",
    "llm_chain = LLMChain(llm=OpenAI(), prompt=prompt)\n",
    "\n",
    "sequential_chain = SimpleSequentialChain(chains=[transform_chain, llm_chain])\n",
    "\n",
    "sequential_chain.invoke(cats_and_dogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 에이전트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import SerpAPIWrapper\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "os.environ[\"SERPAPI_API_KEY\"]\n",
    "\n",
    "search = SerpAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m 첫 번째로 어떤 검색어를 입력해야 할지 고민해볼 수 있습니다.\n",
      "Action: Search\n",
      "Action Input: \"Avatar 2 release date\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mDecember 16, 2022\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m 이 정보는 신뢰할만한 출처에서 가져왔는지 확인해볼 필요가 있습니다.\n",
      "Action: Search\n",
      "Action Input: \"Avatar 2 release date official\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mDecember 16, 2022\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m 확인 결과, 이 정보는 영화 제작사인 20세기 폭스의 공식 홈페이지에서 확인할 수 있었습니다.\n",
      "Final Answer: December 16, 2022\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '아바타 2 개봉일은?', 'output': 'December 16, 2022'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [Tool.from_function(\n",
    "        func=search.run,\n",
    "        name=\"Search\",\n",
    "        description=\"현재 이벤트에 관해 질문할 때 유용함\"\n",
    "    )]\n",
    "\n",
    "agent = initialize_agent(tools, llm = OpenAI(), agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "agent.invoke(\"아바타 2 개봉일은?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 허깅페이스 허브를 통해 LLM 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-huggingface\n",
      "  Downloading langchain_huggingface-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting huggingface-hub>=0.23.0 (from langchain-huggingface)\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.52 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-huggingface) (0.2.9)\n",
      "Collecting sentence-transformers>=2.6.0 (from langchain-huggingface)\n",
      "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting tokenizers>=0.19.1 (from langchain-huggingface)\n",
      "  Downloading tokenizers-0.19.1-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting transformers>=4.39.0 (from langchain-huggingface)\n",
      "  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.7/43.7 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2023.12.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (0.1.81)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (1.10.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (8.2.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.26.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.4.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.12.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (10.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface) (2023.12.25)\n",
      "Collecting safetensors>=0.4.1 (from transformers>=4.39.0->langchain-huggingface)\n",
      "  Downloading safetensors-0.4.4-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain-huggingface) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain-huggingface) (3.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2023.11.17)\n",
      "Requirement already satisfied: sympy in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->langchain-huggingface) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
      "Downloading langchain_huggingface-0.0.3-py3-none-any.whl (17 kB)\n",
      "Downloading huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "   ---------------------------------------- 0.0/417.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 417.5/417.5 kB 8.7 MB/s eta 0:00:00\n",
      "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
      "   ---------------------------------------- 0.0/227.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 227.1/227.1 kB 7.0 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp311-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.9/2.2 MB 19.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 21.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 20.3 MB/s eta 0:00:00\n",
      "Downloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\n",
      "   ---------------------------------------- 0.0/9.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.1/9.5 MB 36.0 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 2.6/9.5 MB 33.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.0/9.5 MB 31.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.4/9.5 MB 31.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.9/9.5 MB 31.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.3/9.5 MB 31.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.5/9.5 MB 31.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.5/9.5 MB 28.8 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.4-cp311-none-win_amd64.whl (285 kB)\n",
      "   ---------------------------------------- 0.0/286.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 286.0/286.0 kB 17.2 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers, sentence-transformers, langchain-huggingface\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.3\n",
      "    Uninstalling huggingface-hub-0.20.3:\n",
      "      Successfully uninstalled huggingface-hub-0.20.3\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.1\n",
      "    Uninstalling tokenizers-0.15.1:\n",
      "      Successfully uninstalled tokenizers-0.15.1\n",
      "Successfully installed huggingface-hub-0.24.5 langchain-huggingface-0.0.3 safetensors-0.4.4 sentence-transformers-3.0.1 tokenizers-0.19.1 transformers-4.44.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#필요한 패키지 설치\n",
    "!pip install python-dotenv   \n",
    "!pip install langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hf_'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#옵션 1: .env 파일에서 토큰 가져기기\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"][:3]  # 앞부분만 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#옵션 2: getpass 함수를 사용해 토큰 가져오기\n",
    "\n",
    "# from getpass import getpass\n",
    "\n",
    "# HUGGINGFACEHUB_API_TOKEN = getpass()\n",
    "# HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "question = \"What was the first Disney movie?\"\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: give a direct answer\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\yong\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "\n",
      "The first Disney movie was 'Snow White and the Seven Dwarfs' released in 1937.\n"
     ]
    }
   ],
   "source": [
    "repo_id = \"tiiuae/falcon-7b-instruct\"  \n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    max_length=1000,\n",
    "    temperature=0.5,\n",
    ")\n",
    "print(llm.invoke(\"what was the first disney movie?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
