{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 랭체인의 구성 요소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델과 프롬프트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sk-'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"][:3]  # 앞부분만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"왜 닭은 도로를 건너야 하나요? 물론, 계란을 횡단보도에 놓고 건너기 때문이죠!\"\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI()\n",
    "print(llm.invoke('농담을 해봐.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장: 탁자 위에 고양이가 있다\n",
      "영어로 번역:\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"문장: {sentence}\n",
    "{language}로 번역:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"sentence\", \"language\"])\n",
    "\n",
    "print(prompt.format(sentence = \"탁자 위에 고양이가 있다\", language = \"영어\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 연결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문서 로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예제 CSV 파일 \"sample.csv\"를 만들었습니다.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    ['이름', '나이', '도시'],\n",
    "    ['존', 25, '뉴욕'],\n",
    "    ['에밀리', 28, '로스엔젤레스'],\n",
    "    ['미카엘', 22, '시카고']\n",
    "]\n",
    "\n",
    "# File name\n",
    "file_name = 'sample.csv'\n",
    "\n",
    "# 데이터를 CSV 파일에 기록\n",
    "with open(file_name, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerows(data)\n",
    "\n",
    "print(f'예제 CSV 파일 \"{file_name}\"를 만들었습니다.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='이름: 존\\n나이: 25\\n도시: 뉴욕', metadata={'source': 'sample.csv', 'row': 0}), Document(page_content='이름: 에밀리\\n나이: 28\\n도시: 로스엔젤레스', metadata={'source': 'sample.csv', 'row': 1}), Document(page_content='이름: 미카엘\\n나이: 22\\n도시: 시카고', metadata={'source': 'sample.csv', 'row': 2})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path='sample.csv')\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문서 분할기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='고요한 풍경 속에서 우뚝 솟은 산들은 자연의 아름다움을 지키는 장엄한 수호자처럼 서 있습니다.'\n",
      "page_content='청량한 산 공기는 고요함의 속삭임을 전해주며, 바스락거리는 잎사귀들은 야생의 교향곡을 작곡합니다.\\n자연의 팔레트는 산을 초록과 갈색의 색조로 칠해 경이로운 광경을 만들어냅니다.'\n",
      "page_content='해가 뜨면, 황금빛 광채가 산 정상에 비치며, 손길 닿지 않은 야생의 세계를 비춥니다.'\n"
     ]
    }
   ],
   "source": [
    "# 산과 자연에 대한 샘플 문장\n",
    "content = \"\"\"고요한 풍경 속에서 우뚝 솟은 산들은 자연의 아름다움을 지키는 장엄한 수호자처럼 서 있습니다.\n",
    "청량한 산 공기는 고요함의 속삭임을 전해주며, 바스락거리는 잎사귀들은 야생의 교향곡을 작곡합니다.\n",
    "자연의 팔레트는 산을 초록과 갈색의 색조로 칠해 경이로운 광경을 만들어냅니다.\n",
    "해가 뜨면, 황금빛 광채가 산 정상에 비치며, 손길 닿지 않은 야생의 세계를 비춥니다.\"\"\"\n",
    "\n",
    "# 파일명\n",
    "file_name = 'mountain.txt'\n",
    "\n",
    "# 텍스트 파일에 내용 쓰기\n",
    "with open(file_name, 'w') as txtfile:\n",
    "    txtfile.write(content)\n",
    "\n",
    "# 샘플 텍스트 파일 \"mountain.txt\" 생성 및 저장.\n",
    "# print(f'샘플 텍스트 파일 \"{file_name}\"를 만들었습니다.')\n",
    "\n",
    "with open('mountain.txt') as f:\n",
    "    mountain = f.read()\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([mountain])\n",
    "print(texts[0])\n",
    "print(texts[1])\n",
    "print(texts[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 임베딩 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베드된 문서:\n",
      "Number of vector: 5; Dimension of each vector: 1536\n",
      "임베드 질의:\n",
      "Dimension of the vector: 1536\n",
      "Sample of the first 5 elements of the vector: [-0.010634176433086395, -0.01016946416348219, -0.0020040736999362707, 0.023065242916345596, -0.026829415932297707]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model ='text-embedding-3-small' )\n",
    "\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"Good morning!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"I want to report an accident\",\n",
    "        \"Sorry to hear that. May I ask your name?\",\n",
    "        \"Sure, Mario Rossi.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"임베드된 문서:\")\n",
    "print(f\"Number of vector: {len(embeddings)}; Dimension of each vector: {len(embeddings[0])}\")\n",
    "\n",
    "embedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\")\n",
    "\n",
    "print(\"임베드 질의:\")\n",
    "print(f\"Dimension of the vector: {len(embedded_query)}\")\n",
    "print(f\"Sample of the first 5 elements of the vector: {embedded_query[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대화 텍스트 파일 \"dialogue.txt\"를 만들었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 대화를 txt 파일에 저장\n",
    "# 대화 행 목록\n",
    "dialogue_lines = [\n",
    "    \"Good morning!\",\n",
    "    \"Oh, hello!\",\n",
    "    \"I want to report an accident\",\n",
    "    \"Sorry to hear that. May I ask your name?\",\n",
    "    \"Sure, Mario Rossi.\"\n",
    "]\n",
    "\n",
    "# 파일명\n",
    "file_name = 'dialogue.txt'\n",
    "\n",
    "# 대화 행들을 텍스트 파일에 기록\n",
    "with open(file_name, 'w') as txtfile:\n",
    "    for line in dialogue_lines:\n",
    "        txtfile.write(line + '\\n')\n",
    "\n",
    "print(f'대화 텍스트 파일 \"{file_name}\"를 만들었습니다.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 벡터 스토어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# 문서를 로드해 청크들로 분할하고, 각 청크를 임베드해 벡터 스토어에 로드\n",
    "\n",
    "raw_documents = TextLoader('dialogue.txt').load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=50, chunk_overlap=0, separator = \"\\n\",)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "db = FAISS.from_documents(documents, OpenAIEmbeddings())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to report an accident\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the reason for calling?\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Sorry to hear that. May I ask your name?' metadata={'source': 'dialogue.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(documents[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검색기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The reason for the call was to report an accident.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "query = \"What was the reason of the call?\"\n",
    "output = qa.invoke(query)\n",
    "output['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 메모리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': '\\nThe human is looking for ideas to write an essay about AI. The AI suggests writing about LLM.'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory, ChatMessageHistory\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=OpenAI(temperature=0))\n",
    "memory.save_context({\"input\": \"안녕하세요, AI에 관한 에세이를 쓸 아이디어를 찾고 있어요\"}, {\"output\": \"안녕하세요, LLM에 관해 써보면 어떨까요?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mConversationSummaryMemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Dict[str, Any]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Dict[str, str]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m'None'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m Save context from this conversation to buffer.\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\langchain\\memory\\summary.py\n",
       "\u001b[1;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConversationSummaryMemory.save_context?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 체인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단순 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yong\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' There is a cat on the table.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate, OpenAI, LLMChain\n",
    "\n",
    "template = \"\"\"문장: {sentence}\n",
    "{language}로 번역:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"sentence\", \"language\"])\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "llm_chain.predict(sentence=\"탁자 위에 고양이가 있어요\", language=\"영어\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There is a cat on the table.\n"
     ]
    }
   ],
   "source": [
    "# LLMChain deprecation 해결\n",
    "\n",
    "from langchain import PromptTemplate, OpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableSequence, RunnablePassthrough\n",
    "\n",
    "template = \"\"\"문장: {sentence}\n",
    "{language}로 번역:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"sentence\", \"language\"])\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = RunnableSequence(\n",
    "    {\n",
    "        \"sentence\": RunnablePassthrough(),\n",
    "        \"language\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "result = chain.invoke({\n",
    "    \"sentence\": \"탁자 위에 고양이가 있어요\",\n",
    "    \"language\": \"영어\"\n",
    "})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라우터 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "itinerary_template = \"\"\"당신은 휴가 일정 조수입니다. \\\n",
    "당신은 고객이 최고의 목적지와 일정을 찾도록 도와줍니다. \\\n",
    "당신은 고객의 선호에 따라 최적화된 일정을 작성하는 데 도움을 줍니다.\n",
    "\n",
    "여기에 질문이 있습니다:\n",
    "{input}\"\"\"\n",
    "\n",
    "restaurant_template = \"\"\"당신은 레스토랑 예약 조수입니다. \\\n",
    "당신은 고객의 손님 수와 음식 선호를 확인합니다. \\\n",
    "특별한 조건을 고려해야 하는지 주의합니다.\n",
    "\n",
    "여기에 질문이 있습니다:\n",
    "{input}\"\"\"\n",
    "\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"여행 일정\",\n",
    "        \"description\": \"여행 일정 작성을 돕습니다\",\n",
    "        \"prompt_template\": itinerary_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"레스토랑\",\n",
    "        \"description\": \"고객의 레스토랑 예약을 도와줍니다\",\n",
    "        \"prompt_template\": restaurant_template,\n",
    "    },\n",
    "]\n",
    "\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "default_chain = ConversationChain(llm=llm, output_key=\"text\")\n",
    "\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "여행 일정: {'input': '밀라노에서 베니스까지 자동차로 여행하려고 합니다. 중간에 들릴 만한 명소가 있나요?.'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "1. 베니스에 가기 전에 들르기 좋은 도시는 어디인가요?\n",
      "2. 베니스에서 머물만한 좋은 호텔은 어디인가요?\n",
      "3. 베니스에서 즐길 수 있는 가장 인기있는 여행지는 어디인가요?\n",
      "4. 베니스에서 가장 유명한 레스토랑이 어디인가요?\n",
      "5. 베니스에서 가장 인기있는 액티비티는 무엇인가요?\n",
      "6. 베니스에서 볼 수 있는 유명한 건축물은 어디인가요?\n",
      "7. 베니스에서 가장 아름다운 풍경을 볼 수 있는 곳은 어디인가요?\n",
      "8. 베니스에서 쇼핑하기 좋은 곳은 어디인가요?\n",
      "9. 베니스에서 가장 유명\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"밀라노에서 베니스까지 자동차로 여행하려고 합니다. 중간에 들릴 만한 명소가 있나요?.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "레스토랑: {'input': '오늘 저녁 식사를 예약하고 싶어요'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      ". 몇 명이서 식사하시겠어요?\n",
      "어떤 종류의 음식을 선호하시나요?\n",
      "특별한 요구사항이 있나요? (알레르기, 식이 요구사항 등)\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"오늘 저녁 식사를 예약하고 싶어요\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시퀀셜 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# This is an LLMChain to write a synopsis given a title of a play.\n",
    "llm = OpenAI(temperature=0)\n",
    "template = \"\"\"당신은 코미디언입니다. {topic}에 관한 농담을 생성하세요.\n",
    "농담:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"topic\"], template=template)\n",
    "joke_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "template = \"\"\"당신은 번역가입니다. 주어진 텍스트 입력을 {language}로 번역하세요.\n",
    "번역:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"language\"], template=template)\n",
    "translator_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m \"고양이와 개가 싸우면 누가 이길까요? 당연히 고양이죠, 개는 꼬리를 쫓는데 바쁘니까!\"\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m If cats and dogs fight, who do you think will win? Of course, it's the cat, because the dog is too busy chasing its tail!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# This is the overall chain where we run these two chains in sequence.\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[joke_chain, translator_chain], verbose=True)\n",
    "translated_joke = overall_chain.run(\"고양이와 개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 변환 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string 모듈을 임포트합니다\n",
    "import string\n",
    "\n",
    "# 함수를 정의합니다\n",
    "def rename_cat(inputs: dict) -> dict:\n",
    "    # 파일을 읽기 모드로 엽니다\n",
    "    text = inputs[\"text\"]\n",
    "    # 'cat'을 'Silvester the Cat'으로 바꿉니다\n",
    "    new_text = text.replace('cat', 'Silvester the Cat')\n",
    "    # 변경된 텍스트를 반환합니다\n",
    "    return {\"output_text\": new_text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 실베스터와 개가 같은 집에서 살았는데 실베스터는 장난을 잘 치고, 개는 충실하고 친근했다. 어느 날 실베스터는 개를 속이기 위해 실을 발을 붙여놓고 숨었다. 개는 이를 알아채고 실베스터를 쫓다가 싸웠지만 주인에게 혼나고 다시 친구가 되었다.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import TransformChain, LLMChain, SimpleSequentialChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "with open(\"Cats&Dogs.txt\") as f:\n",
    "    cats_and_dogs = f.read()\n",
    "\n",
    "\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "transform_chain = TransformChain(\n",
    "    input_variables=[\"text\"], output_variables=[\"output_text\"], transform=rename_cat\n",
    ")\n",
    "\n",
    "template = \"\"\"이 텍스트를 요약하세요:\n",
    "\n",
    "{output_text}\n",
    "\n",
    "요약:\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"output_text\"], template=template)\n",
    "llm_chain = LLMChain(llm=OpenAI(), prompt=prompt)\n",
    "\n",
    "sequential_chain = SimpleSequentialChain(chains=[transform_chain, llm_chain])\n",
    "\n",
    "sequential_chain.run(cats_and_dogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 에이전트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import SerpAPIWrapper\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"SERPAPI_API_KEY\"]\n",
    "\n",
    "search = SerpAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yong\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m 아바타 2는 언제쯤 개봉할까 생각해보자\n",
      "Action:Search\n",
      "Action Input: \"Avatar 2 release date\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mDecember 16, 2022\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m 그렇구나, 아바타 2는 2022년 12월 16일에 개봉한다는 거구나!\n",
      "Final Answer: December 16, 2022\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'December 16, 2022'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [Tool.from_function(\n",
    "        func=search.run,\n",
    "        name=\"Search\",\n",
    "        description=\"현재 이벤트에 관해 질문할 때 유용함\"\n",
    "    )]\n",
    "\n",
    "agent = initialize_agent(tools, llm = OpenAI(), agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "agent.run(\"아바타 2 개봉일은?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 허깅페이스 허브를 통해 LLM 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-huggingface\n",
      "  Downloading langchain_huggingface-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting huggingface-hub>=0.23.0 (from langchain-huggingface)\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.52 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-huggingface) (0.2.9)\n",
      "Collecting sentence-transformers>=2.6.0 (from langchain-huggingface)\n",
      "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting tokenizers>=0.19.1 (from langchain-huggingface)\n",
      "  Downloading tokenizers-0.19.1-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting transformers>=4.39.0 (from langchain-huggingface)\n",
      "  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.7/43.7 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2023.12.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (0.1.81)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (1.10.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (8.2.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.26.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.4.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.12.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (10.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface) (2023.12.25)\n",
      "Collecting safetensors>=0.4.1 (from transformers>=4.39.0->langchain-huggingface)\n",
      "  Downloading safetensors-0.4.4-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain-huggingface) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain-huggingface) (3.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2023.11.17)\n",
      "Requirement already satisfied: sympy in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->langchain-huggingface) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\yong\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
      "Downloading langchain_huggingface-0.0.3-py3-none-any.whl (17 kB)\n",
      "Downloading huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "   ---------------------------------------- 0.0/417.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 417.5/417.5 kB 8.7 MB/s eta 0:00:00\n",
      "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
      "   ---------------------------------------- 0.0/227.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 227.1/227.1 kB 7.0 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp311-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.9/2.2 MB 19.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 21.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 20.3 MB/s eta 0:00:00\n",
      "Downloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\n",
      "   ---------------------------------------- 0.0/9.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.1/9.5 MB 36.0 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 2.6/9.5 MB 33.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.0/9.5 MB 31.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.4/9.5 MB 31.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.9/9.5 MB 31.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.3/9.5 MB 31.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.5/9.5 MB 31.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.5/9.5 MB 28.8 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.4-cp311-none-win_amd64.whl (285 kB)\n",
      "   ---------------------------------------- 0.0/286.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 286.0/286.0 kB 17.2 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers, sentence-transformers, langchain-huggingface\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.3\n",
      "    Uninstalling huggingface-hub-0.20.3:\n",
      "      Successfully uninstalled huggingface-hub-0.20.3\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.1\n",
      "    Uninstalling tokenizers-0.15.1:\n",
      "      Successfully uninstalled tokenizers-0.15.1\n",
      "Successfully installed huggingface-hub-0.24.5 langchain-huggingface-0.0.3 safetensors-0.4.4 sentence-transformers-3.0.1 tokenizers-0.19.1 transformers-4.44.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#필요한 패키지 설치\n",
    "!pip install python-dotenv   \n",
    "!pip install langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hf_'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#옵션 1: .env 파일에서 토큰 가져기기\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"][:3]  # 앞부분만 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#옵션 2: getpass 함수를 사용해 토큰 가져오기\n",
    "\n",
    "# from getpass import getpass\n",
    "\n",
    "# HUGGINGFACEHUB_API_TOKEN = getpass()\n",
    "# HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "question = \"What was the first Disney movie?\"\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: give a direct answer\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\yong\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "\n",
      "The first Disney movie was 'Snow White and the Seven Dwarfs', released in 1937.\n"
     ]
    }
   ],
   "source": [
    "repo_id = \"tiiuae/falcon-7b-instruct\"  \n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    max_length=1000,\n",
    "    temperature=0.5,\n",
    ")\n",
    "print(llm(\"what was the first disney movie?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
